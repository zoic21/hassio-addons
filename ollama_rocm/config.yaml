name: "Ollama ROCM"
description: "Get up and running with large language models."
version: "rocm"
slug: "ollama_rocm"
arch:
  - amd64
url: https://ollama.com
map:
  - config:rw
  - share:rw
image: docker.io/ollama/ollama
init: false
ingress: true
legacy: true
video: true
ingress_port: 11434
ingress_stream: true
ports:
  11434/tcp: 11434
ports_description:
  11434/tcp: "The Ollama API Port, not used for End-User Access"
options:
  OLLAMA_MODELS: /share/ollama_rocm
  OLLAMA_KEEP_ALIVE: -1
  OLLAMA_KV_CACHE_TYPE: q8_0
  OLLAMA_FLASH_ATTENTION: 0
  OLLAMA_MAX_LOADED_MODELS: 1
  OLLAMA_NUM_PARALLEL: 1
  OLLAMA_ORIGINS: ""
  HSA_OVERRIDE_GFX_VERSION: ""
  gpus: all
schema:
  OLLAMA_FLASH_ATTENTION: int
  OLLAMA_MODELS: list(/config/ollama_rocm|/share/ollama_rocm)
  OLLAMA_KEEP_ALIVE: int
  OLLAMA_KV_CACHE_TYPE: list(f16|q8_0|q4_0)
  OLLAMA_MAX_LOADED_MODELS: int
  OLLAMA_NUM_PARALLEL: int
  OLLAMA_ORIGINS: str
  HSA_OVERRIDE_GFX_VERSION: str
  gpus: str
devices:
  - /dev/dri
  - /dev/kfd
startup: application
watchdog: tcp://[HOST]:[PORT:11434]
